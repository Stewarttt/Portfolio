{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed=1\n",
      "Training:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/300 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-159aabcf4583>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mSEEDS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 248\u001b[1;33m             \u001b[0mcurvesThree\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m             \u001b[0mcurves\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mcurvesTwo\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-159aabcf4583>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(seed, type)\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_episode_rb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicyq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m             \u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay_episode_rb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;31m# Train after collecting sufficient experience\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\New folder\\Q2\\utils\\envs.py\u001b[0m in \u001b[0;36mplay_episode_rb\u001b[1;34m(env, policy, buf)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mactions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-159aabcf4583>\u001b[0m in \u001b[0;36mpolicys\u001b[1;34m(env, obs)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[1;31m# With probability EPSILON, choose a random action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;31m# Rest of the time, choose argmax_a Q(s, a)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m     \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[1;31m# Epsilon update rule: Keep reducing a small amount over\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import utils.envs, utils.seed, utils.buffers, utils.torch, utils.common\n",
    "import torch\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Deep Q Learning\n",
    "# Slide 14\n",
    "# cs.uwaterloo.ca/~ppoupart/teaching/cs885-spring20/slides/cs885-lecture4b.pdf\n",
    "\n",
    "# Constants\n",
    "SEEDS = [1,2,3,4,5]\n",
    "t = utils.torch.TorchHelper()\n",
    "DEVICE = t.device\n",
    "OBS_N = 4               # State space size\n",
    "ACT_N = 2               # Action space size\n",
    "MINIBATCH_SIZE = 64     # How many examples to sample per train step\n",
    "GAMMA = 0.99            # Discount factor in episodic reward objective\n",
    "LEARNING_RATE = 5e-4    # Learning rate for Adam optimizer\n",
    "TRAIN_AFTER_EPISODES = 10   # Just collect episodes for these many episodes\n",
    "TRAIN_EPOCHS = 25       # Train for these many epochs every time\n",
    "BUFSIZE = 10000         # Replay buffer size\n",
    "EPISODES = 300          # Total number of episodes to learn over\n",
    "TEST_EPISODES = 1       # Test episodes after every train episode\n",
    "HIDDEN = 512            # Hidden nodes\n",
    "TARGET_UPDATE_FREQ = 10 # Target network update frequency\n",
    "STARTING_EPSILON = 1.0  # Starting epsilon\n",
    "STEPS_MAX = 20000       # Gradually reduce epsilon over these many steps\n",
    "EPSILON_END = 0.01      # At the end, keep epsilon at this value\n",
    "\n",
    "# Global variables\n",
    "EPSILON = STARTING_EPSILON\n",
    "Q = None\n",
    "temperature = 10\n",
    "\n",
    "# Create environment\n",
    "# Create replay buffer\n",
    "# Create network for Q(s, a)\n",
    "# Create target network\n",
    "# Create optimizer\n",
    "def create_everything(seed):\n",
    "\n",
    "    utils.seed.seed(seed)\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    env.seed(seed)\n",
    "    test_env = gym.make(\"CartPole-v0\")\n",
    "    test_env.seed(10+seed)\n",
    "    buf = utils.buffers.ReplayBuffer(BUFSIZE)\n",
    "    Q = torch.nn.Sequential(\n",
    "        torch.nn.Linear(OBS_N, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, ACT_N)\n",
    "    ).to(DEVICE)\n",
    "    Qt = torch.nn.Sequential(\n",
    "        torch.nn.Linear(OBS_N, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, HIDDEN), torch.nn.ReLU(),\n",
    "        torch.nn.Linear(HIDDEN, ACT_N)\n",
    "    ).to(DEVICE)\n",
    "    OPT = torch.optim.Adam(Q.parameters(), lr = LEARNING_RATE)\n",
    "    return env, test_env, buf, Q, Qt, OPT\n",
    "\n",
    "# Update a target network using a source network\n",
    "def update(target, source):\n",
    "    for tp, p in zip(target.parameters(), source.parameters()):\n",
    "        tp.data.copy_(p.data)\n",
    "\n",
    "# Create epsilon-greedy policy\n",
    "def policy(env, obs):\n",
    "\n",
    "    global EPSILON, Q\n",
    "\n",
    "    obs = t.f(obs).view(-1, OBS_N)  # Convert to torch tensor\n",
    "    \n",
    "    # With probability EPSILON, choose a random action\n",
    "    # Rest of the time, choose argmax_a Q(s, a) \n",
    "    if np.random.rand() < EPSILON:\n",
    "        action = np.random.randint(ACT_N)\n",
    "    else:\n",
    "        qvalues = Q(obs)\n",
    "        action = torch.argmax(qvalues).item()\n",
    "    \n",
    "    # Epsilon update rule: Keep reducing a small amount over\n",
    "    # STEPS_MAX number of steps, and at the end, fix to EPSILON_END\n",
    "    EPSILON = max(EPSILON_END, EPSILON - (1.0 / STEPS_MAX))\n",
    "    # print(EPSILON)\n",
    "\n",
    "    return action\n",
    "\n",
    "# Create epsilon-greedy policy\n",
    "def policys(env, obs):\n",
    "\n",
    "    global EPSILON, Q\n",
    "\n",
    "    obs = t.f(obs).view(-1, OBS_N)  # Convert to torch tensor\n",
    "    \n",
    "    qvalues = Q(obs)\n",
    "    \n",
    "    # With probability EPSILON, choose a random action\n",
    "    # Rest of the time, choose argmax_a Q(s, a) \n",
    "    action = torch.multinomial(qvalues, 1).item()\n",
    "    \n",
    "    # Epsilon update rule: Keep reducing a small amount over\n",
    "    # STEPS_MAX number of steps, and at the end, fix to EPSILON_END\n",
    "    EPSILON = max(EPSILON_END, EPSILON - (1.0 / STEPS_MAX))\n",
    "    # print(EPSILON)\n",
    "\n",
    "    return action\n",
    "\n",
    "# Create epsilon-greedy policy\n",
    "def policyq(env, obs):\n",
    "\n",
    "    global EPSILON, Q, temperature\n",
    "\n",
    "    obs = t.f(obs).view(-1, OBS_N)  # Convert to torch tensor\n",
    "    \n",
    "    #Find the action for the qLearning\n",
    "    if EPSILON <= np.random.rand(1) :\n",
    "        action = torch.random.choice(ACT_N)\n",
    "    else :\n",
    "        if temperature == 0 :\n",
    "            qvalues = Q(obs)\n",
    "            action = torch.argmax(qvalues).item()\n",
    "        else :\n",
    "            qvalues = Q(obs)\n",
    "            boltz = torch.exp(qvalues/temperature)\n",
    "            probabilityTotal = boltz / torch.sum(boltz)\n",
    "            if torch.sum(probabilityTotal) > 0 :\n",
    "                action = torch.multinomial(probabilityTotal, 1).item()\n",
    "            else :\n",
    "                action = torch.argmax(qvalues).item()\n",
    "    #END IF\n",
    "\n",
    "    return action\n",
    "\n",
    "# Update networks\n",
    "def update_networks(epi, buf, Q, Qt, OPT):\n",
    "    \n",
    "    # Sample a minibatch (s, a, r, s', d)\n",
    "    # Each variable is a vector of corresponding values\n",
    "    S, A, R, S2, D = buf.sample(MINIBATCH_SIZE, t)\n",
    "    \n",
    "    # Get Q(s, a) for every (s, a) in the minibatch\n",
    "    qvalues = Q(S).gather(1, A.view(-1, 1)).squeeze()\n",
    "\n",
    "    # Get max_a' Qt(s', a') for every (s') in the minibatch\n",
    "    q2values = torch.max(Qt(S2), dim = 1).values\n",
    "\n",
    "    # If done, \n",
    "    #   y = r(s, a) + GAMMA * max_a' Q(s', a') * (0)\n",
    "    # If not done,\n",
    "    #   y = r(s, a) + GAMMA * max_a' Q(s', a') * (1)       \n",
    "    targets = R + GAMMA * q2values * (1-D)\n",
    "\n",
    "    # Detach y since it is the target. Target values should\n",
    "    # be kept fixed.\n",
    "    loss = torch.nn.MSELoss()(targets.detach(), qvalues)\n",
    "\n",
    "    # Backpropagation\n",
    "    OPT.zero_grad()\n",
    "    loss.backward()\n",
    "    OPT.step()\n",
    "\n",
    "    # Update target network every few steps\n",
    "    if epi % TARGET_UPDATE_FREQ == 0:\n",
    "        update(Qt, Q)\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "# Play episodes\n",
    "# Training function\n",
    "def train(seed, type):\n",
    "\n",
    "    global EPSILON, Q\n",
    "    print(\"Seed=%d\" % seed)\n",
    "\n",
    "    # Create environment, buffer, Q, Q target, optimizer\n",
    "    env, test_env, buf, Q, Qt, OPT = create_everything(seed)\n",
    "\n",
    "    # epsilon greedy exploration\n",
    "    EPSILON = STARTING_EPSILON\n",
    "\n",
    "    testRs = []\n",
    "    last25testRs = []\n",
    "    print(\"Training:\")\n",
    "    pbar = tqdm.trange(EPISODES)\n",
    "    for epi in pbar:\n",
    "\n",
    "        # Play an episode and log episodic reward\n",
    "        if type == 0 :\n",
    "            S, A, R = utils.envs.play_episode_rb(env, policy, buf)\n",
    "        elif type == 1 :\n",
    "            S, A, R = utils.envs.play_episode_rb(env, policyq, buf)\n",
    "        elif type == 2:\n",
    "            S, A, R = utils.envs.play_episode_rb(env, policys, buf)\n",
    "        \n",
    "        # Train after collecting sufficient experience\n",
    "        if epi >= TRAIN_AFTER_EPISODES:\n",
    "\n",
    "            # Train for TRAIN_EPOCHS\n",
    "            for tri in range(TRAIN_EPOCHS): \n",
    "                update_networks(epi, buf, Q, Qt, OPT)\n",
    "\n",
    "        # Evaluate for TEST_EPISODES number of episodes\n",
    "        Rews = []\n",
    "        for epj in range(TEST_EPISODES):\n",
    "            if type == 0 :\n",
    "                S, A, R = utils.envs.play_episode(test_env, policy, render = False)\n",
    "            elif type == 1 :\n",
    "                S, A, R = utils.envs.play_episode(test_env, policyq, render = False)\n",
    "            elif type == 2:\n",
    "                S, A, R = utils.envs.play_episode(test_env, policys, render = False)\n",
    "            Rews += [sum(R)]\n",
    "        testRs += [sum(Rews)/TEST_EPISODES]\n",
    "\n",
    "        # Update progress bar\n",
    "        last25testRs += [sum(testRs[-25:])/len(testRs[-25:])]\n",
    "        pbar.set_description(\"R25(%g)\" % (last25testRs[-1]))\n",
    "\n",
    "    # Close progress bar, environment\n",
    "    pbar.close()\n",
    "    print(\"Training finished!\")\n",
    "    env.close()\n",
    "    test_env.close()\n",
    "\n",
    "    return last25testRs\n",
    "\n",
    "# Plot mean curve and (mean-std, mean+std) curve with some transparency\n",
    "# Clip the curves to be between 0, 200\n",
    "def plot_arrays(vars, color, label):\n",
    "    mean = np.mean(vars, axis=0)\n",
    "    std = np.std(vars, axis=0)\n",
    "    plt.plot(range(len(mean)), mean, color=color, label=label)\n",
    "    plt.fill_between(range(len(mean)), np.maximum(mean-std, 0), np.minimum(mean+std,200), color=color, alpha=0.3)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    colour = ['r','b','g','y']\n",
    "    for element in [10]:\n",
    "\n",
    "        # Train for different seeds\n",
    "        curves = []\n",
    "        curvesTwo = []\n",
    "        curvesThree = []\n",
    "\n",
    "        for seed in SEEDS:\n",
    "            #curvesThree += [train(seed, 2)]\n",
    "            curves += [train(seed, 0)]\n",
    "            curvesTwo += [train(seed, 1)]\n",
    "            \n",
    "        temperature = element\n",
    "        print(curves)\n",
    "        plot_arrays(curves, colour.pop(), 'Standard')\n",
    "        plot_arrays(curvesTwo, colour.pop(), 'QLearning')\n",
    "        plt.legend(loc='best')\n",
    "    \n",
    "    plt.show()\n",
    "    plt.savefig('211.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
